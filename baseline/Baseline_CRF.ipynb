{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/h6/rdzp26t50qv8gfx24mwv95z00000gn/T/ipykernel_34708/1550865266.py\", line 2, in <module>\n",
      "    import spacy\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import sklearn_crfsuite\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF, metrics\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der verbleibenden Patient Notes (pn_num): 9901\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation              location  \\\n",
       "0          ['dad with recent heart attcak']           ['696 724']   \n",
       "1             ['mom with \"thyroid disease']           ['668 693']   \n",
       "2                        ['chest pressure']           ['203 217']   \n",
       "3      ['intermittent episodes', 'episode']  ['70 91', '176 183']   \n",
       "4  ['felt as if he were going to pass out']           ['222 258']   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"../nbme-score-clinical-patient-notes/\"\n",
    "MODEL_DIR = \"crf_models_unified\" # Directory to save the unified model\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    features_df = pd.read_csv(f\"{DATA_PATH}features.csv\")\n",
    "    notes_df = pd.read_csv(f\"{DATA_PATH}patient_notes.csv\")\n",
    "    train_df_raw = pd.read_csv(f\"{DATA_PATH}train.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data files: {e}\")\n",
    "    print(f\"Please ensure the data files are located in '{DATA_PATH}' relative to the script.\")\n",
    "    exit()\n",
    "\n",
    "df = train_df_raw.merge(features_df, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "df = df.merge(notes_df, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "df = df[~df['location'].isna() & (df['location'] != '[]')]\n",
    "\n",
    "print(f\"Anzahl der verbleibenden Patient Notes (pn_num): {df['pn_num'].count()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing notes: 100%|██████████| 1000/1000 [00:26<00:00, 37.04it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "grouped = df.groupby(\"pn_num\")\n",
    "data = []\n",
    "\n",
    "def split_locations(loc_str):\n",
    "    if not isinstance(loc_str, str) or loc_str == \"[]\":\n",
    "        return []\n",
    "    locs = literal_eval(loc_str)  # z. B. ['85 99', '126 131;143 151']\n",
    "    all_spans = []\n",
    "    for loc in locs:\n",
    "        sub_spans = loc.split(\";\")  # → ['126 131', '143 151']\n",
    "        all_spans.extend(sub_spans)\n",
    "    return all_spans\n",
    "\n",
    "for pn_num, group in tqdm(grouped, desc=\"Processing notes\"):\n",
    "    text = group.iloc[0][\"pn_history\"]\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    token_starts = [token.idx for token in doc]\n",
    "    token_ends = [token.idx + len(token) for token in doc]\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        spans = split_locations(row[\"location\"])\n",
    "        for span in spans:\n",
    "            try:\n",
    "                start, end = map(int, span.split())\n",
    "            except:\n",
    "                continue\n",
    "            for i, (tok_start, tok_end) in enumerate(zip(token_starts, token_ends)):\n",
    "                if tok_start >= start and tok_end <= end:\n",
    "                    if tags[i] == \"O\":\n",
    "                        tags[i] = f\"B-{row['feature_num']}\"\n",
    "                    else:\n",
    "                        tags[i] = f\"I-{row['feature_num']}\"\n",
    "\n",
    "    data.append({\"tokens\": list(zip(tokens, tags)), \"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = {\n",
    "        \"bias\": 1.0,\n",
    "        \"word.lower()\": word.lower(),\n",
    "        \"word[-3:]\": word[-3:],\n",
    "        \"word[-2:]\": word[-2:],\n",
    "        \"word.isupper()\": word.isupper(),\n",
    "        \"word.istitle()\": word.istitle(),\n",
    "        \"word.isdigit()\": word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i - 1][0]\n",
    "        features.update({\n",
    "            \"-1:word.lower()\": word1.lower(),\n",
    "            \"-1:word.istitle()\": word1.istitle(),\n",
    "            \"-1:word.isupper()\": word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features[\"BOS\"] = True\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i + 1][0]\n",
    "        features.update({\n",
    "            \"+1:word.lower()\": word1.lower(),\n",
    "            \"+1:word.istitle()\": word1.istitle(),\n",
    "            \"+1:word.isupper()\": word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features[\"EOS\"] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# vorbereiten\n",
    "X_all = [sent2features(d[\"tokens\"]) for d in data]\n",
    "y_all = [sent2labels(d[\"tokens\"]) for d in data]\n",
    "texts_all = [d[\"text\"] for d in data]\n",
    "tokens_all = [d[\"tokens\"] for d in data]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test, texts_train, texts_test, tokens_train, tokens_test = train_test_split(\n",
    "    X_all, y_all, texts_all, tokens_all, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 900/900 [00:00<00:00, 1125.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 103041\n",
      "Seconds required: 0.416\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=21.69 loss=680274.08 active=102341 feature_norm=1.00\n",
      "Iter 2   time=64.92 loss=642317.91 active=102309 feature_norm=12.20\n",
      "Iter 3   time=10.84 loss=485821.50 active=98379 feature_norm=9.24\n",
      "Iter 4   time=152.32 loss=351249.98 active=96937 feature_norm=4.41\n",
      "Iter 5   time=43.60 loss=281986.82 active=99455 feature_norm=6.97\n",
      "Iter 6   time=54.70 loss=208771.48 active=100851 feature_norm=7.21\n",
      "Iter 7   time=97.64 loss=206979.88 active=102317 feature_norm=7.34\n",
      "Iter 8   time=86.87 loss=206439.09 active=102840 feature_norm=7.36\n",
      "Iter 9   time=86.95 loss=205770.54 active=102114 feature_norm=7.27\n",
      "Iter 10  time=10.85 loss=205003.75 active=102644 feature_norm=7.45\n",
      "Iter 11  time=10.93 loss=203929.54 active=102371 feature_norm=7.66\n",
      "Iter 12  time=10.87 loss=194979.11 active=98824 feature_norm=12.91\n",
      "Iter 13  time=10.88 loss=185663.74 active=100916 feature_norm=12.39\n",
      "Iter 14  time=10.83 loss=178305.60 active=101430 feature_norm=14.48\n",
      "Iter 15  time=10.82 loss=154746.53 active=101447 feature_norm=23.13\n",
      "Iter 16  time=10.82 loss=131030.30 active=101621 feature_norm=32.93\n",
      "Iter 17  time=10.79 loss=119899.29 active=101631 feature_norm=37.43\n",
      "Iter 18  time=10.83 loss=103168.56 active=100429 feature_norm=47.35\n",
      "Iter 19  time=10.86 loss=98201.89 active=99613 feature_norm=50.12\n",
      "Iter 20  time=10.80 loss=92877.35 active=99782 feature_norm=57.99\n",
      "Iter 21  time=10.90 loss=84813.02 active=100394 feature_norm=59.11\n",
      "Iter 22  time=10.89 loss=80608.30 active=100390 feature_norm=61.60\n",
      "Iter 23  time=10.83 loss=76741.49 active=99968 feature_norm=65.25\n",
      "Iter 24  time=10.84 loss=70662.59 active=99618 feature_norm=69.97\n",
      "Iter 25  time=10.85 loss=66477.84 active=99480 feature_norm=75.46\n",
      "Iter 26  time=10.84 loss=64008.26 active=99612 feature_norm=76.93\n",
      "Iter 27  time=10.78 loss=61241.39 active=99158 feature_norm=82.70\n",
      "Iter 28  time=10.85 loss=59128.38 active=99316 feature_norm=81.63\n",
      "Iter 29  time=10.85 loss=57792.82 active=99338 feature_norm=82.87\n",
      "Iter 30  time=21.57 loss=53735.73 active=92727 feature_norm=88.36\n",
      "Iter 31  time=21.59 loss=52071.83 active=80744 feature_norm=91.41\n",
      "Iter 32  time=10.79 loss=50471.28 active=75445 feature_norm=93.47\n",
      "Iter 33  time=10.79 loss=48768.86 active=72811 feature_norm=95.13\n",
      "Iter 34  time=10.79 loss=46392.96 active=71717 feature_norm=98.95\n",
      "Iter 35  time=10.79 loss=44056.03 active=71319 feature_norm=104.37\n",
      "Iter 36  time=10.80 loss=42780.93 active=71400 feature_norm=105.68\n",
      "Iter 37  time=10.78 loss=40800.76 active=69182 feature_norm=113.70\n",
      "Iter 38  time=10.78 loss=39600.17 active=69396 feature_norm=111.72\n",
      "Iter 39  time=10.81 loss=38972.53 active=69141 feature_norm=113.10\n",
      "Iter 40  time=21.63 loss=37360.49 active=67220 feature_norm=119.23\n",
      "Iter 41  time=10.84 loss=36578.60 active=66827 feature_norm=121.79\n",
      "Iter 42  time=10.85 loss=36365.41 active=66862 feature_norm=121.38\n",
      "Iter 43  time=10.81 loss=35806.89 active=64632 feature_norm=124.16\n",
      "Iter 44  time=10.80 loss=34923.32 active=64111 feature_norm=125.23\n",
      "Iter 45  time=10.80 loss=33684.53 active=61691 feature_norm=130.59\n",
      "Iter 46  time=10.81 loss=32546.05 active=60256 feature_norm=136.12\n",
      "Iter 47  time=10.81 loss=31111.37 active=58606 feature_norm=143.40\n",
      "Iter 48  time=10.81 loss=30217.51 active=56705 feature_norm=154.20\n",
      "Iter 49  time=10.82 loss=29590.04 active=57027 feature_norm=151.29\n",
      "Iter 50  time=10.80 loss=29146.79 active=56585 feature_norm=153.47\n",
      "Iter 51  time=43.39 loss=28919.91 active=55713 feature_norm=156.41\n",
      "Iter 52  time=10.82 loss=27757.19 active=54097 feature_norm=167.73\n",
      "Iter 53  time=10.86 loss=27610.03 active=54394 feature_norm=167.52\n",
      "Iter 54  time=10.76 loss=27403.20 active=54295 feature_norm=168.38\n",
      "Iter 55  time=10.75 loss=27076.12 active=53707 feature_norm=170.74\n",
      "Iter 56  time=10.79 loss=26771.25 active=52019 feature_norm=181.42\n",
      "Iter 57  time=32.41 loss=26367.23 active=51946 feature_norm=180.93\n",
      "Iter 58  time=10.86 loss=26132.35 active=52078 feature_norm=180.82\n",
      "Iter 59  time=22.69 loss=26022.30 active=51900 feature_norm=181.43\n",
      "Iter 60  time=15.26 loss=25560.82 active=50604 feature_norm=186.66\n",
      "Iter 61  time=11.17 loss=25297.36 active=49972 feature_norm=189.77\n",
      "Iter 62  time=11.16 loss=24962.63 active=48467 feature_norm=194.97\n",
      "Iter 63  time=11.11 loss=24876.01 active=47507 feature_norm=197.21\n",
      "Iter 64  time=10.99 loss=24628.27 active=47357 feature_norm=199.22\n",
      "Iter 65  time=10.95 loss=24449.11 active=46885 feature_norm=202.19\n",
      "Iter 66  time=10.96 loss=24275.65 active=46683 feature_norm=203.83\n",
      "Iter 67  time=10.95 loss=24062.97 active=45933 feature_norm=207.86\n",
      "Iter 68  time=10.95 loss=23890.04 active=45172 feature_norm=208.45\n",
      "Iter 69  time=11.06 loss=23746.61 active=44830 feature_norm=209.47\n",
      "Iter 70  time=10.96 loss=23486.77 active=44352 feature_norm=210.20\n",
      "Iter 71  time=10.97 loss=23317.39 active=44027 feature_norm=211.04\n",
      "Iter 72  time=10.96 loss=23168.78 active=43825 feature_norm=211.34\n",
      "Iter 73  time=10.95 loss=23042.26 active=43635 feature_norm=212.35\n",
      "Iter 74  time=10.94 loss=22940.86 active=43595 feature_norm=212.53\n",
      "Iter 75  time=10.96 loss=22835.25 active=43398 feature_norm=213.46\n",
      "Iter 76  time=10.96 loss=22746.43 active=43230 feature_norm=213.73\n",
      "Iter 77  time=10.97 loss=22667.58 active=43101 feature_norm=214.61\n",
      "Iter 78  time=10.96 loss=22600.74 active=42994 feature_norm=214.84\n",
      "Iter 79  time=10.95 loss=22538.62 active=42859 feature_norm=215.65\n",
      "Iter 80  time=10.91 loss=22480.62 active=42723 feature_norm=215.95\n",
      "Iter 81  time=10.83 loss=22427.08 active=42532 feature_norm=216.77\n",
      "Iter 82  time=10.84 loss=22380.38 active=42480 feature_norm=216.99\n",
      "Iter 83  time=10.81 loss=22336.11 active=42376 feature_norm=217.71\n",
      "Iter 84  time=10.83 loss=22292.07 active=42290 feature_norm=217.93\n",
      "Iter 85  time=10.83 loss=22253.89 active=42185 feature_norm=218.62\n",
      "Iter 86  time=10.83 loss=22217.50 active=42110 feature_norm=218.73\n",
      "Iter 87  time=10.81 loss=22185.14 active=42026 feature_norm=219.25\n",
      "Iter 88  time=10.80 loss=22153.82 active=41924 feature_norm=219.31\n",
      "Iter 89  time=10.81 loss=22124.95 active=41845 feature_norm=219.75\n",
      "Iter 90  time=10.81 loss=22097.45 active=41743 feature_norm=219.72\n",
      "Iter 91  time=10.91 loss=22071.10 active=41668 feature_norm=220.14\n",
      "Iter 92  time=10.85 loss=22046.68 active=41589 feature_norm=220.08\n",
      "Iter 93  time=10.84 loss=22022.67 active=41518 feature_norm=220.42\n",
      "Iter 94  time=10.82 loss=22000.88 active=41423 feature_norm=220.34\n",
      "Iter 95  time=10.83 loss=21978.68 active=41383 feature_norm=220.63\n",
      "Iter 96  time=10.81 loss=21959.27 active=41319 feature_norm=220.51\n",
      "Iter 97  time=10.81 loss=21939.05 active=41264 feature_norm=220.80\n",
      "Iter 98  time=10.82 loss=21921.21 active=41245 feature_norm=220.65\n",
      "Iter 99  time=10.81 loss=21901.99 active=41176 feature_norm=220.90\n",
      "Iter 100 time=10.90 loss=21884.81 active=41103 feature_norm=220.77\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 1710.701\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 41103 (103041)\n",
      "Number of active attributes: 10191 (21745)\n",
      "Number of active labels: 224 (224)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.033\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['crf_baseline_model.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm=\"lbfgs\",\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True,\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "joblib.dump(crf, \"crf_baseline_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (weighted): 0.8680956495966081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         B-0       0.89      0.47      0.62        36\n",
      "         B-1       0.96      1.00      0.98        48\n",
      "        B-10       0.87      0.65      0.74        60\n",
      "       B-100       0.00      0.00      0.00         2\n",
      "       B-101       0.00      0.00      0.00         2\n",
      "       B-102       1.00      0.16      0.27        19\n",
      "       B-103       0.43      0.57      0.49        23\n",
      "       B-104       0.43      0.50      0.46        12\n",
      "       B-105       0.75      0.83      0.79        18\n",
      "       B-106       1.00      0.50      0.67        26\n",
      "       B-107       0.89      0.76      0.82        21\n",
      "       B-108       0.00      0.00      0.00        15\n",
      "       B-109       0.20      0.08      0.11        13\n",
      "        B-11       0.55      0.65      0.60        26\n",
      "       B-110       0.62      0.62      0.62        21\n",
      "       B-111       0.90      0.85      0.88        33\n",
      "       B-112       0.50      0.50      0.50         6\n",
      "        B-12       0.54      0.64      0.58        11\n",
      "         B-2       1.00      0.78      0.88        32\n",
      "       B-200       0.69      0.27      0.39        91\n",
      "       B-201       1.00      0.19      0.32        26\n",
      "       B-202       1.00      1.00      1.00         5\n",
      "       B-203       0.00      0.00      0.00        28\n",
      "       B-204       0.83      1.00      0.91        10\n",
      "       B-205       0.70      0.97      0.81        38\n",
      "       B-206       1.00      0.45      0.62        11\n",
      "       B-207       0.00      0.00      0.00        15\n",
      "       B-208       0.70      0.78      0.74         9\n",
      "       B-210       0.79      0.60      0.68        25\n",
      "       B-211       1.00      1.00      1.00        18\n",
      "       B-212       0.91      0.62      0.74       174\n",
      "       B-213       0.85      0.77      0.81        52\n",
      "       B-214       1.00      0.38      0.55        16\n",
      "       B-215       0.00      0.00      0.00        10\n",
      "       B-216       0.82      0.90      0.86        20\n",
      "         B-3       0.47      0.36      0.41        42\n",
      "       B-300       0.97      0.94      0.95        31\n",
      "       B-301       0.79      0.93      0.85        28\n",
      "       B-302       0.94      0.61      0.74        28\n",
      "       B-303       0.86      0.71      0.77        17\n",
      "       B-304       1.00      0.78      0.88         9\n",
      "       B-305       0.83      1.00      0.91        10\n",
      "       B-306       0.39      0.25      0.30        28\n",
      "       B-307       0.22      0.21      0.22        19\n",
      "       B-308       1.00      0.91      0.95        11\n",
      "       B-309       0.97      0.83      0.90        36\n",
      "       B-310       1.00      0.24      0.38        21\n",
      "       B-311       1.00      0.23      0.38        13\n",
      "       B-312       1.00      0.33      0.50        15\n",
      "       B-313       0.75      0.50      0.60        54\n",
      "       B-314       0.50      0.43      0.46         7\n",
      "       B-315       0.86      1.00      0.92        18\n",
      "         B-4       0.36      0.36      0.36        14\n",
      "       B-400       0.00      0.00      0.00        30\n",
      "       B-401       0.73      0.73      0.73        15\n",
      "       B-402       1.00      0.58      0.74        24\n",
      "       B-403       0.50      0.67      0.57         9\n",
      "       B-404       0.43      0.17      0.24        18\n",
      "       B-405       0.00      0.00      0.00         6\n",
      "       B-406       0.93      0.88      0.90        16\n",
      "       B-407       1.00      0.83      0.91         6\n",
      "       B-408       0.38      0.50      0.43        16\n",
      "       B-409       0.80      0.92      0.86        13\n",
      "         B-5       0.50      0.15      0.23        27\n",
      "       B-500       0.80      0.43      0.56        47\n",
      "       B-501       1.00      0.69      0.82        13\n",
      "       B-502       1.00      0.67      0.80         6\n",
      "       B-503       0.50      0.27      0.35        22\n",
      "       B-504       0.54      0.45      0.49        29\n",
      "       B-505       0.60      0.38      0.47        79\n",
      "       B-506       0.50      0.25      0.33        12\n",
      "       B-507       0.00      0.00      0.00        35\n",
      "       B-508       0.14      0.17      0.15         6\n",
      "       B-509       0.59      0.31      0.40        85\n",
      "       B-510       0.45      0.64      0.53        22\n",
      "       B-511       0.75      0.56      0.64        43\n",
      "       B-512       0.50      0.69      0.58        13\n",
      "       B-513       0.80      0.47      0.59        17\n",
      "       B-514       1.00      0.66      0.79        32\n",
      "       B-515       1.00      0.33      0.50        12\n",
      "       B-516       0.80      0.65      0.72        68\n",
      "       B-517       0.83      0.89      0.86        27\n",
      "         B-6       1.00      0.80      0.89        15\n",
      "       B-600       0.70      0.44      0.54        16\n",
      "       B-601       0.67      0.50      0.57        12\n",
      "       B-602       0.72      0.56      0.63        32\n",
      "       B-603       0.72      0.52      0.60        54\n",
      "       B-604       0.83      0.72      0.77        53\n",
      "       B-605       0.97      0.90      0.93        39\n",
      "       B-606       0.94      0.84      0.89        38\n",
      "       B-607       0.96      0.79      0.87        33\n",
      "       B-608       1.00      0.27      0.43        22\n",
      "       B-609       0.88      0.88      0.88        16\n",
      "       B-610       0.86      0.60      0.71        40\n",
      "       B-611       0.77      0.62      0.69        16\n",
      "         B-7       0.69      0.64      0.67        14\n",
      "       B-700       0.75      0.86      0.80         7\n",
      "       B-701       0.96      0.92      0.94        25\n",
      "       B-702       0.46      0.40      0.43        72\n",
      "       B-703       0.64      0.61      0.62        23\n",
      "       B-704       0.43      0.36      0.39        28\n",
      "       B-705       0.40      0.50      0.44         4\n",
      "       B-706       0.89      0.39      0.54        41\n",
      "       B-707       1.00      0.95      0.97        19\n",
      "       B-708       0.64      0.50      0.56        28\n",
      "         B-8       1.00      0.60      0.75         5\n",
      "       B-800       0.82      0.69      0.75        13\n",
      "       B-801       0.91      0.55      0.69        56\n",
      "       B-802       1.00      0.78      0.88         9\n",
      "       B-803       0.88      0.88      0.88         8\n",
      "       B-804       1.00      1.00      1.00         3\n",
      "       B-805       0.96      1.00      0.98        23\n",
      "       B-806       0.81      0.85      0.83        20\n",
      "       B-808       0.14      0.15      0.15        20\n",
      "       B-809       1.00      1.00      1.00         4\n",
      "       B-810       0.69      0.42      0.52        26\n",
      "       B-811       0.93      0.67      0.78        21\n",
      "       B-812       0.60      0.82      0.69        11\n",
      "       B-813       0.90      0.39      0.55        23\n",
      "       B-814       1.00      0.83      0.90        23\n",
      "       B-815       1.00      0.62      0.76        13\n",
      "       B-816       0.92      0.48      0.63        23\n",
      "       B-817       0.79      0.78      0.79        69\n",
      "         B-9       0.70      0.45      0.55        47\n",
      "       B-900       0.35      0.35      0.35        17\n",
      "       B-901       0.56      0.56      0.56        18\n",
      "       B-902       0.67      0.25      0.36        16\n",
      "       B-903       0.00      0.00      0.00         1\n",
      "       B-904       0.71      0.31      0.43        16\n",
      "       B-905       1.00      0.50      0.67         8\n",
      "       B-906       0.80      0.57      0.67         7\n",
      "       B-907       0.00      0.00      0.00         6\n",
      "       B-908       0.67      0.25      0.36         8\n",
      "       B-909       0.86      0.80      0.83        15\n",
      "       B-910       0.50      1.00      0.67         3\n",
      "       B-912       0.50      0.10      0.16        21\n",
      "       B-913       0.29      0.33      0.31         6\n",
      "       B-914       0.91      1.00      0.95        10\n",
      "       B-915       0.75      0.64      0.69        14\n",
      "       B-916       0.57      0.67      0.62        12\n",
      "       I-105       1.00      0.67      0.80         3\n",
      "       I-108       0.00      0.00      0.00         2\n",
      "       I-110       0.00      0.00      0.00         0\n",
      "       I-111       0.00      0.00      0.00         0\n",
      "       I-200       0.36      0.25      0.30        16\n",
      "       I-201       0.00      0.00      0.00         3\n",
      "       I-205       0.00      0.00      0.00         3\n",
      "       I-206       0.00      0.00      0.00         4\n",
      "       I-207       0.00      0.00      0.00         7\n",
      "       I-211       0.00      0.00      0.00         2\n",
      "       I-212       0.00      0.00      0.00         9\n",
      "       I-213       0.00      0.00      0.00         0\n",
      "       I-214       0.00      0.00      0.00         4\n",
      "       I-315       0.00      0.00      0.00         2\n",
      "       I-400       0.00      0.00      0.00         2\n",
      "       I-401       0.00      0.00      0.00         1\n",
      "       I-402       0.00      0.00      0.00         2\n",
      "       I-404       1.00      0.50      0.67         2\n",
      "       I-405       0.00      0.00      0.00         3\n",
      "       I-409       0.00      0.00      0.00         2\n",
      "         I-5       0.00      0.00      0.00         1\n",
      "       I-505       0.00      0.00      0.00        19\n",
      "       I-507       0.00      0.00      0.00         1\n",
      "       I-508       0.00      0.00      0.00         3\n",
      "       I-509       0.00      0.00      0.00        12\n",
      "       I-512       0.50      0.13      0.21        15\n",
      "       I-513       0.00      0.00      0.00         6\n",
      "       I-515       0.00      0.00      0.00         3\n",
      "       I-611       1.00      0.17      0.29         6\n",
      "       I-701       1.00      1.00      1.00         3\n",
      "       I-702       0.00      0.00      0.00         3\n",
      "       I-808       0.25      0.18      0.21        11\n",
      "       I-813       0.00      0.00      0.00         4\n",
      "       I-816       0.00      0.00      0.00         1\n",
      "       I-817       1.00      0.25      0.40         4\n",
      "         I-9       0.00      0.00      0.00         1\n",
      "       I-900       0.62      0.36      0.45        14\n",
      "       I-904       0.00      0.00      0.00         2\n",
      "       I-912       0.75      1.00      0.86         6\n",
      "           O       0.91      0.97      0.94     14008\n",
      "\n",
      "    accuracy                           0.88     17553\n",
      "   macro avg       0.59      0.46      0.50     17553\n",
      "weighted avg       0.87      0.88      0.87     17553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/finetune/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "print(\"F1 Score (weighted):\", metrics.flat_f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(metrics.flat_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predicted_locations(labels, doc):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, tag in enumerate(labels):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if start is not None:\n",
    "                end = doc[i - 1].idx + len(doc[i - 1])\n",
    "                spans.append(f\"{start} {end}\")\n",
    "            start = doc[i].idx\n",
    "        elif tag == \"O\" and start is not None:\n",
    "            end = doc[i - 1].idx + len(doc[i - 1])\n",
    "            spans.append(f\"{start} {end}\")\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        end = doc[-1].idx + len(doc[-1])\n",
    "        spans.append(f\"{start} {end}\")\n",
    "    return spans\n",
    "\n",
    "def parse_true_spans(label_seq, doc):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, tag in enumerate(label_seq):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            start = doc[i].idx\n",
    "        elif tag == \"O\" and start is not None:\n",
    "            end = doc[i - 1].idx + len(doc[i - 1])\n",
    "            spans.append(f\"{start} {end}\")\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        end = doc[-1].idx + len(doc[-1])\n",
    "        spans.append(f\"{start} {end}\")\n",
    "    return spans\n",
    "\n",
    "def jaccard_score(gt_spans, pred_spans):\n",
    "    gt_chars = set()\n",
    "    pred_chars = set()\n",
    "    for span in gt_spans:\n",
    "        s, e = map(int, span.split())\n",
    "        gt_chars.update(range(s, e))\n",
    "    for span in pred_spans:\n",
    "        s, e = map(int, span.split())\n",
    "        pred_chars.update(range(s, e))\n",
    "    if not gt_chars and not pred_chars:\n",
    "        return 1.0\n",
    "    if not gt_chars or not pred_chars:\n",
    "        return 0.0\n",
    "    return len(gt_chars & pred_chars) / len(gt_chars | pred_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Jaccard: 100%|██████████| 100/100 [00:02<00:00, 38.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittlicher Jaccard Score: 0.2979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i in tqdm(range(len(X_test)), desc=\"Evaluating Jaccard\"):\n",
    "    text = texts_test[i]\n",
    "    sent = tokens_test[i]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    pred = y_pred[i]\n",
    "    true = y_test[i]\n",
    "\n",
    "    pred_spans = extract_predicted_locations(pred, doc)\n",
    "    true_spans = parse_true_spans(true, doc)\n",
    "\n",
    "    score = jaccard_score(true_spans, pred_spans)\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Durchschnittlicher Jaccard Score: {sum(scores)/len(scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
